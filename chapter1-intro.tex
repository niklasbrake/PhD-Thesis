\chapter{Introduction and literature review}

Electrophysiological measurements of neural tissue allow direct observations of millisecond changes in neural activity \cite{Donoghue2020}, on a broad range of spatial scales from micrometers to centimeters. At these large spatial scales, electrophysiological measurements can be made entirely noninvasively through the recording of electrical potentials at the scalp. Over the last century, such scalp recordings, also known as electroencephalography (EEG), have allowed us to measure ongoing neural activity in healthy humans, as well as those suffering from neurological disease, proving itself to be a uniquely important tool for understanding the human brain. Extracting as much information as possible from these signals despite their low spatial resolution is thus highly desirable. Broadly speaking, a fundamental question is: given a difference between two EEG signals, either collected at two timepoints or from two different subjects, how much can we infer about the underlying differences in neurophysiology? 

EEG often exhibits oscillations, which are known to reflect rhythmic synchronized activity in the brain. In recent years, attention has also been devoted to the broadband trends observed in the spectra of EEG signals. In many other physical systems, such properties typically reflect second order statistical properties of the system. In EEG, it remains unclear what these broadband signals represent. Some have argued that these signals are an epiphenomenon of oscillatory brain rhythms. Others have suggested it reflects an entirely different dynamical regime of neural networks. What is the neurophysiology that underlies these broadband EEG signals? In other physical systems, the spectral trend often needs to be explicitly modelled to correct for broadband contamination of signals of interested. In EEG, it is unclear whether this is the case. Does the spectral trend contaminate brain rhythm estimates? If so, how should these signals be corrected for?

In this thesis, I address these questions through the development of biophysical and statistical models of EEG generation that integrate physiological details gleaned over the past two decades. This modelling work updates our understanding of the electric fields generated by the brain, especially as it pertains to broadband and aperiodic EEG signals. 


The physical laws that govern electric fields were described by Maxwell in the 1800s, and as such were understood well before the invention of EEG. Moreover, since the first EEG recordings, electrode and amplifier technology has improved drastically, allowing us to record signals with low noise and frequency resolutions well above that required to measure almost any neural activity. Computational power as exploded in the last decades allowing highly complex and intensive analyses of EEG signals. With these facts taken into account, it is believed that most of the remaining limits on the information available from EEG recordings are principally imposed, not by ignorance of physical laws, nor the engineering of recording systems, but primarily by our understanding of the physiological systems themselves underlying the generation of EEG signals. What do these signals represent biologically? What configurations of neural populations produce a given EEG signal?

\newpage

\section{Existing theories on the spectral trend of macroscopic neural recordings}
\subsection{Local versus global synchrony}

\subsection{Self-organized criticality}
Of all the theories reviewed here, that of self-organized criticality and power-law scaling is perhaps the most elaborated, controversial, and passionately stated. This is probably in part due to a “vague and mistakenly mystical sense of universality”, to quote Stumpf and Porter’s perspective piece in Science (2012), and in part due to the theory’s genuinely elegant implications for brain function, such as optimizing dynamic range (Kinouchi & Copelli. Nature Physics 2006) and information transmission (Shriki and Yellin. PLOS Comput Biol 2016).. 

Many natural systems appear to exhibit power-law scaling, from earthquakes to forest fires. By a power law, it is meant that some physical quantity or probability distribution, $P(S)$ obeys $P(S)\propto x^{-\beta}$. About forty years ago Bak et al. proposed an explanation for these phenomena (Bak et al. Phys Rev Lett 1987). It was known that physical systems near a phase transition exhibited power-law scaling behavior in both time and space. But the parameter regime in which this phase transition is infinitesimally small; how could systems everywhere be perched at this delicate point? The theory of self-organized criticality illustrates that certain systems spontaneously tend towards this transition or “critical” point, by virtue of the complex interactions of the system’s constituents. In other words, this critical point is a stable state of the system. Bak et al. exemplified this with a simple cellular automaton model. Cells are laid out in a lattice and each is associated with a real number value. If a cell’s value exceeds an arbitrary threshold, then a value of one is transferred from the cell to each of its neighbors. When initializing values randomly, the system will evolve to a stable state, but one which is extremely sensitive to perturbation. Tripping just one of the cells will send off a cascade, whose size was shown to be distributed as a power law. In his book, How Nature Works, Per Bak argues that “the complex phenomena observed everywhere indicate that nature operates at the self-organized critical state.”

The parallels between the original cellular automaton model and neural networks are quite apparent: when a neuron’s excitatory input reaches a certain threshold, the cell fires an action potential, which causes an excitatory potential in each of the cell’s postsynaptic partners (assuming all neurons are excitatory) (Corral et al. Phys Rev Lett 1995; Hertz and Hopfield. Phys Rev Lett 1995). Therefore, there was much interest in the theory of self-organized criticality amongst physicists studying the brain. Whereas original studies on simulated neural networks need to tune parameter values to achieve a critical process, it was later shown (Levina et al. Nature Physics 2007) that a network of excitatory integrate-and-fire neurons will self-organize into a critical state with the addition of synaptic depression, the phenomenon whereby repeated firing drains the releasable pool of presynaptic vesicles (Markram and Tsodyks. Nature 1996; Regehr. Cold Spring Harb Perspect Biol 2012). Interestingly, it was also shown that a network of integrate and fire neurons exhibit avalanche criticality when excitation and inhibition is balanced (Poil et al. J. Neurosci. 2012; Lombardi et al. Chaos 2017). 

Due to the potential for power-law scaling in neural dynamics, it has been suggested that self-organized criticality also underlies the power-law scaling observed in the spectra of macroscopic recordings including LFP, EEG, MEG, and fMRI. 

So what evidence is there that the brain is indeed operating in a self-organized critical state?

At the turn of the millennium, Beggs and Plenz tested whether slices of brain tissue exhibit cascades of action potentials that obey the dynamics of a self-organized state (Beggs and Plenz. J. Neurosci. 2003). Organotypic (~28 days in vitro) and acute slices were studied on an electrode array and negative deflections at each electrode, indicative of locally synchronized spiking, were analyzed. These experiments were the first to show that cortical circuits can exhibit so-called neural avalanches – cascades of activity with sizes and durations that obey power laws. Indeed, it was found that the activity was consistent with a branching process at criticality (described below). Later, neuronal avalanches were also identified in LFP recordings in monkeys in vivo, indicating that this phenomenon may indeed play a role in brain dynamics (Petermann et al. PNAS 2009). Power-law relations have since been identified across many species using many different recording modalities, including 2-photon and light-sheet microscopy, LFP, EEG, MEG, and fMRI.

As a branching process will be modelled in the next chapter, I will briefly elaborate on its nature. Consider a population of $N$ individuals. A branching process describes a specific kind of evolution of these individuals, whereby at each timepoint or generation, an individual can produce a certain number of offspring. In a neural network, these individuals are taken to be action potentials and the offspring are the subsequent action potentials elicited in postsynaptic partners. Sometimes, the individuals are taken to be neurons that are either firing (1) or quiescent (0) and the offspring are the postsynaptic partners that begin firing in response to the presynaptic neuron. The dynamics of a branching process is determined by the branching number, which reflects the average number of offspring in each generation. Clearly, if the average number of offspring is greater than 1, then the population size, i.e., neuronal spiking will increase exponentially and unbounded. If the branching number is less than one, the population size will exponentially decay. Thus, a parameter value of 1 is a critical point of the system. This process becomes interesting when it is driven by noise, because as the branching number approaches 1 from below, $m\rightarrow1^{-}$, the cascades induced by the noise grow in size and duration with a distribution that approaches a power law with a slope of -3/2, exactly that observed by Beggs and Plenz.

One caveat to these studies is the lack of strong statistical evidence for power laws. For example, to conclude there is a power relationship between two variables, or in a probability distribution, this relationship should continue over at least two decades of both variables (Stumpf and Porter. Science 2012).This would mean that a spectral trend should exhibit a clear power relationship between, say 1 and 100 Hz in EEG data. Secondly, there should be an actual statistical test of whether the relationship is best explained by a power law. Few of the foregoing studies performed robust statistical analyses on their avalanche distributions (Clauset et al. SIAM Rev. 2009). Worse still, a look through the literature on EEG spectra reveals a hodge-podge of power law measurements, which typically rely on either a visual comparison to a straight line on a log-log plot or at most least-squares fitting, and are often over extremely short segments of spectra: ???????????. Moreover, since EEG spectra clearly exhibit peaks due to narrowband neural activity, it is unclear how these peaks should be accounted for when measuring purported power laws. Often, the spectra are averaged over long periods of time, thus blurring the peaks to the point of nonrecognition.

A second caveat to these studies is that the nature of their measurements alway severely undersample the neuronal populations in vivo, which prevents accurate inference of power laws and branching numbers (Priesemann et al. BMC Neurosci 2009). Recent theoretical advances have provided statistical tools for accurately inferring branching numbers for undersampled systems (Wilting and Priesemann. Nature Commun 2018). Applying these statistical tools to in vivo measurements have revealed that the cortex does not operate at criticality, but rather in a subcritical regime, with a branching number consistently calculated to be subcritical, between approximately 0.94 and 0.998 (Wilting and Priesemann. Nature Commun 2018; Wilting and Priesemann. Cerebral Cortex 2019; Suryadi et al. eNeuro 2022). Interestingly, it has been noted that the purported computational properties that are maximized at criticality also come with trade-offs, such as poor reliability () and the slowing down of calculations (). Therefore, it may be that a slightly subcritical dynamical regime is in fact the best at balancing these various computational properties of the cortex (Wilting and Priesemann. Current Opinion in Neurobiology 2019).

What does this all mean for the EEG spectral trend? What is clear from all the above evidence is that the cortex can operate in an aperiodic regime characterized by cascades of neural activity, regardless if these cascades obey a power law. This evidence suggests that neural synchrony can occur outside the specific dynamical regime of a brain rhythm, and could therefore generate macroscopic, aperiodic electric fields. Indeed, despite often-cited analyses, it is not at all clear that the EEG spectral trend actually follows a power law. Thus, even if such a mechanism lacks a splash of “mystical universality”, the role of neuronal cascades or “reverberating activity” in shaping EEG spectra is one worth investigating further. 




\subsection{Tissue filtering}
In a series of papers, Bédard et al. set out to show theoretically that electrical signals measured in LFP recordings are filtered by the extracellular medium. In the first of these papers, these authors derive a model from first principles whereby Maxwell’s equations are considered under spherical symmetry, but spatially inhomogeneous conductivity \cite{Bedard2004}. Through numerical simulations of this model, the first study found that the extracellular medium may act as a low or high pass filter, depending on the precise spatial organization of conductivity values. In particular, it was found that an exponentially decaying inhomogeneity allows for a low-pass filtering phenomenology. However, with periodic high and low conductivity regions, modelling alternating layers of cerebral spinal fluid and cellular membrane, there was no frequency filtering observed in their simulations. Because of this inconsistency, the model was later elaborated by considering polarizing effects, whereby the source potentials generated by neural activity polarize the membranes of nearby passive cells, such as glia \cite{Bedard2006a}. These polarized membranes then generate their own “induced” electric field. This induced field re-equilibrates as ionic charges redistribute, which occurs with exponentially decaying dynamics characterized by a time constant defined by the conductance and permittivity of the space near the passive cell’s membrane. The authors argue that, as a result of this induced field, fluctuations in the source potentials are ostensibly filtered with an exponential transfer function, thus giving rise to a Lorentzian frequency profile in the power spectrum

While these theories could describe qualitatively the $1/f$ scaling observed in LFP recordings, there had been no attempt to quantitatively match experimental measurements. This was achieved in a subsequent paper \cite{Bedard2009}, where the electric properties of the neural tissue were explicitly modeled as frequency dependent, in line with experimental measurements \cite{Gabriel1996}. Interestingly, quantitative comparison with these experimental measurements of brain tissue conductivity suggested that membrane polarization only plays a role in filtering signals below approximately 1 Hz. This result indicated that their previous model \cite{Bedard2006a} could not explain the majority of the broadband spectral trend in LFP data. Instead, Bédard and Destexhe\cite{Bedard2006a} introduced ionic diffusion into their model; this ion diffusion added an additional $1/\sqrt{f}$ filter to the signal propagating in the extracellular medium. Interestingly, this mechanism implies that the frequency dependence of tissue conductivity observed by Gabriel et al.\cite{Gabriel1996} resulted from ion accumulation at the electrode-electrolyte interface, which is known to be frequency dependent \cite{Warburg1899}, and may therefore be an artifact of the measuring system.

Prior to this last paper, Logothesis et al. had developed a novel setup for measuring brain conductivity in primates in vivo and concluded that neural tissue is almost entirely resistive \cite{Logothetis2007}, i.e.,there is no filtering by the extracellular medium in contrast to the observations of Gabriel et al.\cite{Gabriel1996}. In particular, by using a four electrode system, the electrode pair used to measure voltage was not subject to charge accumulation and therefore did not suffer from the confounding effects in the Gabriel et al. experiments. The observations of Lo gothetis et al. have since been verified by many subsequent studies (reviewed by Pesaran et al.\cite{Pesaran2018}). This observation was also modelled by Bédard and Destexhe and the authors showed that the experimental results indeed agreed with a system absent of ionic diffusion \cite{Bedard2006a}. However, the authors argued that under true physiological conditions, ionic diffusion should play a role in the generation of electric fields due in part to the redistribution of ions after the activation of ion channels.The exact magnitude with which this phenomenon may contribute under physiological conditions has yet to be determined.

These studies all focused on LFP recordings and therefore concerned themselves entirely with the filtering of neural tissue, whereas EEG signals must also pass through the scalp and skull. These structures are reported to have frequency-dependent conductivities, but this filtering is seemingly minor at the frequencies relevant to EEG \cite{Pfurtscheller1975, Akhtari2002, Pesaran2018}. Moreover, differences in this frequency-dependence across individuals seem to be insignificant \cite{Akhtari2002}, and changes in this filtering over time is highly unlikely. 

The extent to which tissue properties cause 1/f scaling in macroscopic electric measurements is still debated \cite{Bedard2017}. However, present evidence suggests that the effects are likely minimal. Importantly, even if frequency dependent filtering plays a small role in determining the overall scaling of EEG recordings, these filtering effects would not be expected to vary significantly over time or across individuals. Therefore, these mechanisms would not be able to explain differences in the broadband component of EEG spectra. 

\subsection{Synaptic timescales}

