\chapter{Introduction and literature review}

Electrophysiological measurements of neural tissue allow direct observations of millisecond changes in neural activity \cite{Donoghue2020}, on a broad range of spatial scales from micrometers to centimeters. At these large spatial scales, electrophysiological measurements can be made entirely noninvasively through the recording of electrical potentials at the scalp. Over the last century, such scalp recordings, also known as electroencephalography (EEG), have allowed us to measure ongoing neural activity in healthy humans, as well as those suffering from neurological disease, proving itself to be a uniquely important tool for understanding the human brain. Extracting as much information as possible from these signals despite their low spatial resolution is thus highly desirable. Broadly speaking, a fundamental question is: given a difference between two EEG signals, either collected at two timepoints or from two different subjects, how much can we infer about the underlying differences in neurophysiology? 

EEG often exhibits oscillations, which are known to reflect rhythmic synchronized activity in the brain. In recent years, attention has also been devoted to the broadband trends observed in the spectra of EEG signals. In many other physical systems, such properties typically reflect second order statistical properties of the system. In EEG, it remains unclear what these broadband signals represent. Some have argued that these signals are an epiphenomenon of oscillatory brain rhythms. Others have suggested it reflects an entirely different dynamical regime of neural networks. What is the neurophysiology that underlies these broadband EEG signals? In other physical systems, the spectral trend often needs to be explicitly modelled to correct for broadband contamination of signals of interested. In EEG, it is unclear whether this is the case. Does the spectral trend contaminate brain rhythm estimates? If so, how should these signals be corrected for?

In this thesis, I address these questions through the development of biophysical and statistical models of EEG generation that integrate physiological details gleaned over the past two decades. This modelling work updates our understanding of the electric fields generated by the brain, especially as it pertains to broadband and aperiodic EEG signals. 


The physical laws that govern electric fields were described by Maxwell in the 1800s, and as such were understood well before the invention of EEG. Moreover, since the first EEG recordings, electrode and amplifier technology has improved drastically, allowing us to record signals with low noise and frequency resolutions well above that required to measure almost any neural activity. Computational power as exploded in the last decades allowing highly complex and intensive analyses of EEG signals. With these facts taken into account, it is believed that most of the remaining limits on the information available from EEG recordings are principally imposed, not by ignorance of physical laws, nor the engineering of recording systems, but primarily by our understanding of the physiological systems themselves underlying the generation of EEG signals. What do these signals represent biologically? What configurations of neural populations produce a given EEG signal?

\newpage

\section{Existing theories on the spectral trend of macroscopic neural recordings}
\subsection{Local versus global synchrony: the classical view}

\subsection{Synaptic timescales: unchallenged and underexplored}

\subsection{Filtering through neural issue: persistent and dissentient}
In a series of papers, Bédard et al.\cite{Bedard2004,Bedard2006a,Bedard2009} set out to show theoretically that electrical signals measured in LFP recordings are filtered by the extracellular medium. In the first of these papers, these authors derive a model from first principles whereby Maxwell’s equations are considered under spherical symmetry, but spatially inhomogeneous conductivity \cite{Bedard2004}. Through numerical simulations of this model, the first study found that the extracellular medium may act as a low or high pass filter, depending on the precise spatial organization of conductivity values. In particular, it was found that an exponentially decaying inhomogeneity allows for a low-pass filtering phenomenology. However, with periodic high and low conductivity regions, modelling alternating layers of cerebral spinal fluid and cellular membrane, there was no frequency filtering observed in their simulations. Because of this inconsistency, the model was later elaborated by considering polarizing effects, whereby the source potentials generated by neural activity polarize the membranes of nearby passive cells, such as glia \cite{Bedard2006a}. These polarized membranes then generate their own “induced” electric field. This induced field re-equilibrates as ionic charges redistribute, which occurs with exponentially decaying dynamics characterized by a time constant defined by the conductance and permittivity of the space near the passive cell’s membrane. The authors argue that, as a result of this induced field, fluctuations in the source potentials are ostensibly filtered with an exponential transfer function, thus giving rise to a Lorentzian frequency profile in the power spectrum

While these theories could describe qualitatively the $1/f$ scaling observed in LFP recordings, there had been no attempt to quantitatively match experimental measurements. This was achieved in a subsequent paper \cite{Bedard2009}, where the electric properties of the neural tissue were explicitly modeled as frequency dependent, in line with experimental measurements \cite{Gabriel1996}. Interestingly, quantitative comparison with these experimental measurements of brain tissue conductivity suggested that membrane polarization only plays a role in filtering signals below approximately 1 Hz. This result indicated that their previous model \cite{Bedard2006a} could not explain the majority of the broadband spectral trend in LFP data. Instead, Bédard and Destexhe\cite{Bedard2006a} introduced ionic diffusion into their model; this ion diffusion added an additional $1/\sqrt{f}$ filter to the signal propagating in the extracellular medium. Interestingly, this mechanism implies that the frequency dependence of tissue conductivity observed by Gabriel et al.\cite{Gabriel1996} resulted from ion accumulation at the electrode-electrolyte interface, which is known to be frequency dependent \cite{Warburg1899}, and may therefore be an artifact of the measuring system.

Prior to this last paper, Logothesis et al.\cite{Logothetis2007} had developed a novel setup for measuring brain conductivity in primates in vivo and concluded that neural tissue is almost entirely resistiv , i.e.,there is no filtering by the extracellular medium in contrast to the observations of Gabriel et al.\cite{Gabriel1996}. In particular, by using a four electrode system, the electrode pair used to measure voltage was not subject to charge accumulation and therefore did not suffer from the confounding effects in the Gabriel et al. experiments. The observations of Logothetis et al. have since been verified by many subsequent studies (reviewed by Pesaran et al.\cite{Pesaran2018}). This observation was also modelled by Bédard and Destexhe\cite{Bedard2006a} and the authors showed that the experimental results indeed agreed with a system absent of ionic diffusion. However, the authors argued that under true physiological conditions, ionic diffusion should play a role in the generation of electric fields due in part to the redistribution of ions after the activation of ion channels.The exact magnitude with which this phenomenon may contribute under physiological conditions has yet to be determined.

These studies all focused on LFP recordings and therefore concerned themselves entirely with the filtering of neural tissue, whereas EEG signals must also pass through the scalp and skull. These structures are reported to have frequency-dependent conductivities, but this filtering is seemingly minor at the frequencies relevant to EEG \cite{Pfurtscheller1975, Akhtari2002, Pesaran2018}. Moreover, differences in this frequency-dependence across individuals seem to be insignificant \cite{Akhtari2002}, and changes in this filtering over time is highly unlikely. 

The extent to which tissue properties cause 1/f scaling in macroscopic electric measurements is still debated \cite{Bedard2017}. However, present evidence suggests that the effects are likely minimal. Importantly, even if frequency dependent filtering plays a small role in determining the overall scaling of EEG recordings, these filtering effects would not be expected to vary significantly over time or across individuals. Therefore, these mechanisms would not be able to explain differences in the broadband component of EEG spectra. 

\subsection{Self-organized criticality: evidence and controversy}
Of all the theories reviewed here, that of self-organized criticality and power-law scaling is perhaps the most elaborated, controversial, and passionately stated. This is probably in part due to a “vague and mistakenly mystical sense of universality”, to quote Stumpf and Porter’s perspective piece in \textit{Science}\cite{Stumpf2012}, and in part due to the theory’s genuinely elegant implications for brain function, such as optimizing dynamic range \cite{Kinouchi2006} and information transmission \cite{Shriki2016}. 

Many natural systems appear to exhibit power-law scaling, from earthquakes to forest fires. By a power law, it is meant that some physical quantity or probability distribution, $P(S)$, obeys $P(S)\propto S^{-\beta}$. Forty years ago, a now seminal paper by Bak et al. \cite{Bak1987} proposed an explanation for this seemingly universal phenomena. It was known that physical systems near a phase transition exhibited power-law scaling behavior in both time and space. But the parameter regime of this phase transition is infinitesimally small; how could systems everywhere be perched at this delicate point? The theory of self-organized criticality illustrates that certain systems spontaneously tend towards this critical point, by virtue of the complex interactions of the system’s constituents. In other words, this critical point is a stable state of the system. Bak et al. exemplified this with a simple cellular automaton model. Cells are laid out in a lattice and each is associated with a real number value. If a cell’s value exceeds an arbitrary threshold, then a value of one is transferred from the cell to each of its neighbors. When initializing values randomly, the system will evolve to a stable state, but one which is extremely sensitive to perturbation. Tripping just one of the cells will send off a so-called “avalanche”, whose size was shown to be distributed as a power law. In his book, \textit{How Nature Works}\cite{Bak1996}, Per Bak argues that “the complex phenomena observed everywhere indicate that nature operates at the self-organized critical state.”

The parallels between the original cellular automaton model and neural networks are quite apparent: when a neuron’s excitatory input reaches a certain threshold, the cell fires an action potential, which causes an excitatory potential in each of the cell’s postsynaptic partners (assuming all neurons are excitatory). Therefore, there was much interest in the theory of self-organized criticality amongst physicists studying the brain \cite{Corral1995, Herz1995}. While these original studies on simulated neural networks needed to tune parameter values to achieve a critical process, it was later shown \cite{Levina2007} that a network of excitatory integrate-and-fire neurons will self-organize into a critical state with the addition of synaptic depression, the phenomenon whereby repeated firing drains the releasable pool of presynaptic vesicles \cite{Markram1996, Regehr2012}. Interestingly, it was also shown that networks of excitatory and inhibitory integrate-and-fire neurons exhibit avalanche criticality when excitation and inhibition is particularly balanced \cite{Poil2012, Lombardi2017}. Thus, many studies indicate that self-organized criticality is theoretically possible in neural networks.

Due to the potential for power-law scaling in neural dynamics, it has been suggested that self-organized criticality also underlies the broadband spectrum of macroscopic recordings including LFP and EEG \cite{Lombardi2017}. Indeed, the relationship between the size and duration of avalanches is thought to determine the scaling of the power spectrum \cite{Kuntz2000}: the average height ($S$) of an avalanche is related to its duration ($T$) as a power law $S(T)\propto T^{1/\sigma\nu z}$, and thus the exponent of the power spectrum is $\beta=1/\sigma\nu z$ (the exact meaning of the parameters $\sigma$, $\nu$, and $z$ are not relevant for our discussion but are defined by Kuntz and Sethna \cite{Kuntz2000}). Simulated networks of excitatory and inhibition neurons at criticality can generate neuronal avalanches with power-law spectra that appear similar to the spectra of macroscopic electrical potentials \cite{Lombardi2017}.

So what evidence is there that the brain is operating in a self-organized critical state?

At the turn of the millennium, Beggs and Plenz \cite{Beggs2003} tested whether slices of brain tissue exhibit cascades of action potentials that obey the dynamics of a self-organized state. Organotypic (~28 days in vitro) and acute slices were studied on an electrode array and negative deflections at each electrode, indicative of locally synchronized spiking, were analyzed. These experiments were the first to show that cortical circuits can exhibit so-called neural avalanches – cascades of activity with sizes and durations that obey power laws. Indeed, it was found that the activity was consistent with a branching process (described below, see \hyperref[sec:branching_prcoess]{\textit{Definition of branching process}}) at criticality. Later, neuronal avalanches consistent with a branching process were also identified in LFP recordings in monkeys in vivo, indicating that this phenomenon may indeed play a role in physiological brain dynamics \cite{Petermann2009}. Power-law relations have since been identified across many species using many different recording modalities, including voltage \cite{Scott2014} and calcium imaging \cite{Bellay2015,Ponce-Alvarez2018}.

Despite the explosion of papers on self-organized criticality in the brain over the past two decades, the topic remains highly controversial. One caveat to these studies is the lack of strong statistical evidence for power laws. For example, to conclude there is a power relationship between two variables, or in a probability distribution, this relationship should continue over at least two decades of both variables and should be estimated with maximum likelihood estimation \cite{Stumpf2012}. Secondly, there should be an actual statistical test of whether the relationship is best explained by a power law. Only a couple of the foregoing studies performed robust statistical analyses on their avalanche distributions \cite{Clauset2009}.The most convincing analyses are from calcium imaging studies \cite{Bellay2015, Ponce-Alvarez2018}. However, due to the slow kinetics of calcium indicators, the avalanche power spectrum can only be computed for very low frequencies, less than 1 Hz in the case of Ponce-Alvarez et al.\cite{Ponce-Alvarez2018}, and therefore unfortunately do not provide much information on the EEG spectral trend. 

This caveat also plays a role in tying self-organized criticality to EEG, as a look through the literature on EEG spectra reveals a hodge-podge of power law measurements, which typically rely on least-squares fitting (not maximum likelihood) and are often over extremely short segments of spectra: $\beta=-1.3$ from 0.5 to 30 Hz \cite{Pritchard1992}; $\beta=-1.4$ from 0.15 to 9.5 Hz \cite{Dehghani2010}; $\beta=-2$ from 30 to 50 Hz \cite{Lendner2020}; $\beta -1$ from 1 to 40 Hz \cite{Colombo2019}; $\beta\in[-3,-1]$ from 3 to 30 Hz \cite{Pereda1998}. This is by no means an exhaustive list. One likely reason for these varied and often short frequency ranges is the difficulty in estimating power laws from the whole spectrum; EEG spectra clearly exhibit peaks due to narrowband neural activity, and it is unclear how these peaks should be accounted for when measuring purported power laws. These peaks are either entirely ignored and included in the power-law fit \cite{Pritchard1992}, or the spectra are averaged over such long periods of time that the peaks are blurred to the point of nonrecognition. Above 50 Hz, even a “short” time window of 20 s ($\beta\in[-3,-2]$ from 1 to 100 Hz \cite{He2010}) contains at least 1000 periods of an oscillation. As the peak frequency of a gamma oscillation can vary significantly from cycle to cycle (REF), there is very high potential for the blurring of spectral peaks and scaling at these frequencies may therefore be an epiphenomenon of purely rhythmic activity. Indeed, such a blurring of spectral peaks at various frequencies is essentially the argument outlined in \hyperref[sec:branching_prcoess]{\textit{Local versus global synchrony: the classical view}} and is clearly not consistent with self-organized criticality. In sum, the evidence of power-law scaling in EEG spectra is inconsistent and the actual value of the spectral exponent is very poorly defined. 

A second caveat to assessing criticality in the brain is that the nature of in vivo electrophysiological measurements severely undersamples the neuronal population, which prevents accurate inference of power laws and branching numbers \cite{Priesemann2009}. Even imaging studies often only genetically express calcium or voltage indicators in a subset of neurons, such as layer 2/3 pyramidal cells \cite{Scott2014,Bellay2015}. Other studies were performed in transgenic zebrafish \cite{Ponce-Alvarez2018} which despite expressing GCaMP under a pan-neuronal promoter often display incomplete expression. Therefore, even measurements of ''whole-brain dynamics'' still suffer from subsampling. Recent theoretical advances have provided statistical tools for accurately inferring branching numbers from undersampled systems \cite{Wilting2018}. Applying these statistical tools to in vivo electrophysiological recordings have suggested that the cortex does not operate at criticality, but rather in a subcritical regime, with a branching number consistently calculated to be subcritical, between approximately 0.94 and 0.998 in monkeys, cats, mice \cite{Wilting2018,Wilting2019} and zebrafish \cite{Suryadi2022}. Interestingly, it has been noted that the purported computational properties that are maximized at criticality also come with trade-offs, such as poor reliability \cite{Gollo2017} and critical slowing down \cite{Scheffer2012, Wilting2019a}. Therefore, it has been suggested that a slightly subcritical dynamical regime may in fact balance the various computational tasks of the cortex better than it would at criticality \cite{Wilting2019a}.

What does this all mean for the EEG spectral trend? What is clear from all the above evidence is that the cortex can operate in an aperiodic regime characterized by cascades of neural activity, regardless if these cascades obey a power law consistent with criticality. Indeed, what is more important than criticality is that neural synchrony can occur outside the specific dynamical regime of a brain rhythm, and therefore in theory might generate macroscopic, aperiodic electric fields. Even if such a mechanism lacks a splash of “mystical universality”, the role of neuronal cascades, reverberating activity, or simply spike-propagation in shaping EEG spectra is one worth investigating further. In particular, might there be cortical dynamics other than oscillations that can generate detectable scalp potentials? Even if this activity does not generate power-law scaling in EEG spectra, might it still contribute somehow to the spectral trend?

\subsubsection{Definition of branching process} \label{sec:branching_prcoess}
I will briefly elaborate on the nature of branching processes as they are employed in the modelling presented here in this thesis. Consider a population of individuals. A branching process (with immigration) describes a specific kind of evolution of these individuals, whereby at each generation, an individual can produce a certain number of offspring. In a neural network, these individuals are taken to be action potentials and the offspring are the subsequent action potentials elicited in postsynaptic partners. The dynamics of a branching process is determined by the branching number, $m$, which reflects the average number of offspring in each generation, and the amount of immigration, i.e., the number of externally added individuals at each generation. Mathematically, the population size at a given time point is given by the following recursive equation \cite{Wilting2018}
\begin{equation}
    A_{t+1} = \sum_{i=1}^{A_{t}} y_{i,t} + h_t
\end{equation}
where $y_{i,t}\sim\mathcal{Y}$ are independent and identically distributed according to some law $\mathcal{Y}$, with mean $\mathbb{E}(\mathcal{Y})=m$. If the branching number is $m>1$, then the population size, i.e. neuronal spiking, will increase exponentially and unbounded. If the branching number is $m<1$, the population size will reach a stable steady-state, $A_{\infty}=h/(1-m)$. Thus, a parameter value of $m=1$ is a critical point of the system. This process becomes interesting when it is driven by noise, i.e., $h_t$ are independently and identically distributed, non-negative integer valued random variables, because the number of individuals will exhibit avalanches with a size distribution that approaches a power law with a slope of $-3/2$ as $m\to1^-$, exactly that observed by Beggs and Plenz \cite{Beggs2003}.

\subsection{Summary}